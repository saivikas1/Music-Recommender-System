# -*- coding: utf-8 -*-
"""
MusicRecomendationSystemSVD.ipynb1.ipynb
Automatically generated by Colaboratory.
"""
#pip install surprise
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.sparse import csr_matrix
from surprise import SVD
from surprise import Dataset, Reader
from surprise import accuracy
from surprise.model_selection import train_test_split
from surprise.model_selection import GridSearchCV
from surprise.model_selection import cross_validate

# Loading the dataset
SongsDataFrame = pd.read_csv('song_data.csv')

# Data Cleaning
# removing duplicate song records
SongsDataFrame.drop_duplicates(['song_id'], inplace=True)

# loading song meta data
SongsMetaData = pd.read_csv('10000.txt',sep='\t',header=None)
SongsMetaData.columns = ['user_id', 'song_id', 'listen_count']

# merging song meta data df with songs dataframe
df_merged = pd.merge(SongsMetaData, SongsDataFrame, on="song_id", how="left")

# finding song listen count by a distinct users by grouping on song id, user id columns
song_listencount = df_merged.groupby('song_id')['user_id'].count()

# plotting the fraction of songs listened vs number of users
plt.figure(figsize=(16, 8))
sns.distplot(song_listencount.values, color='yellow')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.show()

print(f"A song is listened by {np.mean(song_listencount)} users on average, with minimum {np.min(song_listencount)} and maximum {np.max(song_listencount)} users")


UserCount = df_merged.groupby('user_id')['song_id'].count()
# Filter users who listened to at least 10 songs
max_user_filer = UserCount[UserCount > 10].index.to_list()
# Get how many user have listened to each song
song_listen_frequency = df_merged.groupby('song_id')['user_id'].count()
# Get songs which have been listened at least 4 times
song_filter_id_4 = song_listen_frequency[song_listen_frequency > 18].index.to_list()
reduced_df = df_merged[(df_merged['user_id'].isin(max_user_filer)) & (df_merged['song_id'].isin(song_filter_id_4))].reset_index(drop=True)
reduced_df.shape

"""# Implementing Singular Value Decomposition
(Matrix Factorization) using the rating with binning technique
"""

# binning the song listen frequencies from 1 to 10 for each song for each user
bins = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2214]
reduced_df['listen_count']
reduced_df['listen_count'] = pd.cut(reduced_df['listen_count'], bins=bins, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
listen_counts = pd.DataFrame(reduced_df.groupby('listen_count').size(), columns=['count']).reset_index(drop=False)
listen_counts

# plotting 'the number of songs vs listen frequency'
plt.figure(figsize=(16, 8))
sns.barplot(x='listen_count', y='count', palette='Set1', data=listen_counts)
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.show()

reader = Reader(rating_scale=(1, 10))
data = Dataset.load_from_df(reduced_df[['user_id', 'song_id', 'listen_count']], reader)

# bifucating the data into train and test data
trainset, testset = train_test_split(data, test_size=.3)

"""# Grid Search Parameters"""
# hyper parameter tuning
hyper_parameters = {'n_factors': [120],
                    'n_epochs': [100],
                    'lr_all': [0.003, 0.001],
                    'reg_all': [0.08, 0.1]
                    }
GridSearch = GridSearchCV(SVD, hyper_parameters, measures=['rmse'], cv=3, joblib_verbose=4, n_jobs=-2)
GridSearch.fit(data)
estimator = GridSearch.best_estimator['rmse']
print(GridSearch.best_score['rmse'])
print(GridSearch.best_params['rmse'])

# performing cross validation
cross_validate(estimator, data, measures=['RMSE'], cv=5, verbose=True)

# performing singular value decomposition
svd_fitting = SVD(n_factors=120, n_epochs=100, lr_all=0.001, reg_all=0.1)
svd_fitting.fit(trainset)
test_predictions = svd_fitting.test(testset)
print(f"The RMSE is {accuracy.rmse(test_predictions, verbose=True)}")